{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example in NLP\n",
    "\n",
    "In this notebook, we will do sentiment analysis on the SST-2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data, datasets, vocab\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print (torch.__version__)\n",
    "print (torchtext.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already downloaded the dataset and stored in the folder called 'SST-2'. There are three files: train/dev.tsv. \n",
    "\n",
    "Note that the labels for the test are not given, hence we will just use the dev set for evaluation.\n",
    "\n",
    "Each line in the data file consists of a sentence and a label. Label 1 means the sentence is positive, 0 means the sentence is negative.\n",
    "\n",
    "Let's first load the datset and see how the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True, include_lengths=True)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "train_dataset, dev_dataset = data.TabularDataset.splits(\n",
    "    path='SST-2/', format='tsv', skip_header=True,\n",
    "    train='train.tsv', validation='dev.tsv',\n",
    "    fields=[('sentence', TEXT), ('label', LABEL)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train examples:  67349\n",
      "#dev examples:  872\n",
      "{'sentence': ['equals', 'the', 'original', 'and', 'in', 'some', 'ways', 'even', 'betters', 'it'], 'label': '1'}\n",
      "{'sentence': ['pumpkin', 'takes', 'an', 'admirable', 'look', 'at', 'the', 'hypocrisy', 'of', 'political', 'correctness', ',', 'but', 'it', 'does', 'so', 'with', 'such', 'an', 'uneven', 'tone', 'that', 'you', 'never', 'know', 'when', 'humor', 'ends', 'and', 'tragedy', 'begins', '.'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# number of examples\n",
    "print ('#train examples: ', len(train_dataset))\n",
    "print ('#dev examples: ', len(dev_dataset))\n",
    "\n",
    "# check one example\n",
    "print (vars(train_dataset.examples[20]))\n",
    "print (vars(dev_dataset.examples[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.409553222765\n",
      "19.548165137614678\n"
     ]
    }
   ],
   "source": [
    "# average sentence length\n",
    "\n",
    "print (sum([len(vars(eg)['sentence']) for eg in train_dataset.examples])/len(train_dataset))\n",
    "print (sum([len(vars(eg)['sentence']) for eg in dev_dataset.examples])/len(dev_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For standard ML practice, you should set aside a test set from the dev set, train and tune models on the trian and dev set only, and then evaluate the trained model on the test set only.\n",
    "\n",
    "But for simplicity, I'll skip that for now. We'll cover more in the lectures.\n",
    "\n",
    "Unlike images, for text data, we tokenize the data into words, and we need to build up vocab for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Vocab:  14818\n",
      "Label Vocab:  2\n"
     ]
    }
   ],
   "source": [
    "# build our vocab based on data\n",
    "TEXT.build_vocab(train_dataset,)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "\n",
    "print ('Text Vocab: ', len(TEXT.vocab))\n",
    "print ('Label Vocab: ', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 27205), (',', 25980), ('a', 21609), ('and', 19920), ('of', 17907), ('.', 12687), ('to', 12538), (\"'s\", 8764), ('is', 8685), ('that', 7759), ('in', 7495), ('it', 7078), ('as', 5088), ('with', 4745), ('an', 4133), ('film', 4038), ('its', 3924), ('for', 3913), ('movie', 3563), ('this', 3365)]\n"
     ]
    }
   ],
   "source": [
    "# view some frequent words in vocab\n",
    "print (TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1})\n"
     ]
    }
   ],
   "source": [
    "print (LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding in RNN for batched training is tricky.\n",
    "Please refer to this post: https://zhuanlan.zhihu.com/p/34418001 for more details.\n",
    "\n",
    "It's ok if you don't get it. We'll cover it in lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.LSTM(input_size=emb_dim,\n",
    "                           hidden_size=hid_dim,\n",
    "                           num_layers=2,\n",
    "                           bidirectional=True)\n",
    "        self.fc = nn.Linear(hid_dim*2, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        # input: (bsz, sent_len)->(bsz, sent_len, emb_dim)\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        \n",
    "        # output: (sen_len, bsz, hid_dim*num_direction)\n",
    "        # hidden: (num_layer*num_direction, bsz, hid_dim)\n",
    "        # concat the final forward and backward final hidden states\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1,:,:]), dim=1)\n",
    "        # hidden: (bsz, hid_dim*2)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, dev_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, dev_dataset),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.sentence),\n",
    "    sort_within_batch=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "vocab_size = len(TEXT.vocab)\n",
    "emb_dim = 50\n",
    "hid_dim = 64\n",
    "model = RNN(vocab_size, emb_dim, hid_dim)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def binary_accuracy(preds, y):\n",
    "    preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds==y).float()\n",
    "    acc = correct.sum() / len(correct) \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3\n",
    "def train(model, iterator, epoch):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.sentence\n",
    "        preds = model(text, text_lengths).squeeze(1)\n",
    "        loss = loss_fn(preds, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print (\"Epoch: {}, Batch Idx: {}, Loss: {}\".format(\n",
    "                    epoch, batch_idx, loss.item()))\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.sentence\n",
    "            preds = model(text, text_lengths).squeeze(1)\n",
    "            acc = binary_accuracy(preds, batch.label)\n",
    "            \n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_acc / len(iterator) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a timer\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    mins = int(elapsed_time / 60)\n",
    "    secs = int(elapsed_time - (mins)*60)\n",
    "    print (\"Time: {} mins {} secs\".format(mins, secs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch Idx: 0, Loss: 0.6879485249519348\n",
      "Epoch: 1, Batch Idx: 200, Loss: 0.6326194405555725\n",
      "Epoch: 1, Batch Idx: 400, Loss: 0.5964372158050537\n",
      "Epoch: 1, Batch Idx: 600, Loss: 0.6924533247947693\n",
      "Epoch: 1, Batch Idx: 800, Loss: 0.3474215269088745\n",
      "Epoch: 1, Batch Idx: 1000, Loss: 0.48663467168807983\n",
      "Time: 1 mins 39 secs\n",
      "Epoch: 1, Acc: 76.5625\n",
      "Epoch: 2, Batch Idx: 0, Loss: 0.48060038685798645\n",
      "Epoch: 2, Batch Idx: 200, Loss: 0.37727341055870056\n",
      "Epoch: 2, Batch Idx: 400, Loss: 0.41725897789001465\n",
      "Epoch: 2, Batch Idx: 600, Loss: 0.5222943425178528\n",
      "Epoch: 2, Batch Idx: 800, Loss: 0.2690412402153015\n",
      "Epoch: 2, Batch Idx: 1000, Loss: 0.31534940004348755\n",
      "Time: 1 mins 33 secs\n",
      "Epoch: 2, Acc: 79.53124982970101\n",
      "Epoch: 3, Batch Idx: 0, Loss: 0.3253956437110901\n",
      "Epoch: 3, Batch Idx: 200, Loss: 0.3124137818813324\n",
      "Epoch: 3, Batch Idx: 400, Loss: 0.44053900241851807\n",
      "Epoch: 3, Batch Idx: 600, Loss: 0.19532600045204163\n",
      "Epoch: 3, Batch Idx: 800, Loss: 0.2438146024942398\n",
      "Epoch: 3, Batch Idx: 1000, Loss: 0.24055926501750946\n",
      "Time: 1 mins 29 secs\n",
      "Epoch: 3, Acc: 80.08928554398673\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 3\n",
    "\n",
    "for epoch in range(1, EPOCH+1):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    train_loss = train(model, train_iterator, epoch)\n",
    "    dev_acc = evaluate(model, dev_iterator)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time(start_time, end_time)\n",
    "    \n",
    "    print (\"Epoch: {}, Acc: {}\".format(epoch, dev_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used a simple LSTM model, you should see the performance go above random (50%). You are encouraged to run for more epochs, change the hyper-parameters or model structure and use other tricks to further improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model so you can load it later\n",
    "SAVE_PATH = \"sst_rnn.ckpt\"\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "\n",
    "## load the model \n",
    "model = model = RNN(vocab_size, emb_dim, hid_dim)\n",
    "model.load_state_dict(torch.load(SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a predict function to do prediction\n",
    "def predict(model, sent):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sent_ = sent.lower().split()\n",
    "        sent = []\n",
    "        for w in sent_:\n",
    "            if w in TEXT.vocab.stoi:\n",
    "                sent.append([TEXT.vocab.stoi[w]])\n",
    "            else:\n",
    "                sent.append([0])\n",
    "        text_length = torch.LongTensor([len(sent)])\n",
    "        text = torch.LongTensor(sent)\n",
    "        preds = model(text, text_length).squeeze(1)\n",
    "        preds = torch.round(torch.sigmoid(preds)).item()\n",
    "        if preds > 0.5:\n",
    "            print (\"Predicted: Negative\")\n",
    "        else:\n",
    "            print (\"Predicted: Positive\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Negative\n"
     ]
    }
   ],
   "source": [
    "sent = \"I feel kind of sad though\"\n",
    "predict(model, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Positive\n"
     ]
    }
   ],
   "source": [
    "sent = \"Thanks buddy! I had a lot of fun learning this course !!\"\n",
    "predict(model, sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the sentence above and see if the model predicts it right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://gitee.com/quarky/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
