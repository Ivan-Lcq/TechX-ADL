{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1: Basic Operations in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From NumPy to PyTorch: Warm-up\n",
    "\n",
    "Before introducing PyTorch, we will first implement a simple network using numpy (which you should already be familiar with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.03146907157777922\n",
      "100 0.023576430866366595\n",
      "200 0.01946546755435352\n",
      "300 0.01698451471119245\n",
      "400 0.01525402655616587\n",
      "500 0.013902384772600595\n",
      "600 0.012768044170719778\n",
      "700 0.01177944867622355\n",
      "800 0.010899417635858784\n",
      "900 0.010106695280478223\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple feedforward network with ReLU activation.\n",
    "\n",
    "N: batch size,\n",
    "D_in: input dimension\n",
    "H: hidden dimension\n",
    "D_out: output dimension\n",
    "'''\n",
    "N, D_in, H, D_out = 64, 100, 200, 5\n",
    "\n",
    "# create random input and output data\n",
    "x = np.random.randn(N, D_in) # input\n",
    "y = np.random.randn(N, D_out) # output (i.e., labels)\n",
    "\n",
    "'''\n",
    "Randomly initialize weights, bias terms are ignored\n",
    "warning: don't initialize the weights too large, \n",
    "otherwise you may get nan :o\n",
    "'''\n",
    "w1 = np.random.randn(D_in, H)*0.01\n",
    "w2 = np.random.randn(H, D_out)*0.01\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# run for 1000 steps\n",
    "for t in range(1000):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1) # shape: (N, H)\n",
    "    h_relu = np.maximum(h, 0) # add relu activation\n",
    "    y_pred = h_relu.dot(w2) # shape: (N, D_out)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred, y).sum()\n",
    "    if t%100 == 0:\n",
    "        print (t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0*(y_pred - y) # shape: (N, D_out)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # shape: (H, D_out)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # shape: (N, H)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0 # hidden state values < 0 have no grad due to ReLU\n",
    "    grad_w1 = x.T.dot(grad_h) # shape: (D_in, H)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1 \n",
    "    w2 -= learning_rate * grad_w2 \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Natural Language Processing with PyTorch. O'REALLY. Delip Rao & Brian McMahan.\n",
    "\n",
    "Official PyTorch Tutorials: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
