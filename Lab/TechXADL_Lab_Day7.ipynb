{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuGjvLJkRbpQ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/gdrive')\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWo_mB3WRjQV"
   },
   "outputs": [],
   "source": [
    "!wget -c http://images.cocodataset.org/zips/train2014.zip\n",
    "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "!unzip annotations_trainval2014.zip \n",
    "!unzip train2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQNVXTWiSieO"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "import os.path\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "nltk.download('punkt')\n",
    "from torchvision import transforms\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRqpj3AoRon7"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self,\n",
    "        vocab_threshold,\n",
    "        vocab_file='./vocab.pkl',\n",
    "        start_word=\"<start>\",\n",
    "        end_word=\"<end>\",\n",
    "        unk_word=\"<unk>\",\n",
    "        annotations_file='/content/annotations/captions_train2014.json',\n",
    "        vocab_from_file=False):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "          vocab_threshold: Minimum word count threshold.\n",
    "          vocab_file: File containing the vocabulary.\n",
    "          start_word: Special word denoting sentence start.\n",
    "          end_word: Special word denoting sentence end.\n",
    "          unk_word: Special word denoting unknown words.\n",
    "          annotations_file: Path for train annotation file.\n",
    "          vocab_from_file: If False, create vocab from scratch & override any existing vocab_file\n",
    "                           If True, load vocab from from existing vocab_file, if it exists\n",
    "        \"\"\"\n",
    "        self.vocab_threshold = vocab_threshold\n",
    "        self.vocab_file = vocab_file\n",
    "        self.start_word = start_word\n",
    "        self.end_word = end_word\n",
    "        self.unk_word = unk_word\n",
    "        self.annotations_file = annotations_file\n",
    "        self.vocab_from_file = vocab_from_file\n",
    "        self.get_vocab()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Load the vocabulary from file OR build the vocabulary from scratch.\"\"\"\n",
    "        if os.path.exists(self.vocab_file) & self.vocab_from_file:\n",
    "            with open(self.vocab_file, 'rb') as f:\n",
    "                vocab = pickle.load(f)\n",
    "                self.word2idx = vocab.word2idx\n",
    "                self.idx2word = vocab.idx2word\n",
    "            print('Vocabulary successfully loaded from vocab.pkl file!')\n",
    "        else:\n",
    "            self.build_vocab()\n",
    "            with open(self.vocab_file, 'wb') as f:\n",
    "                pickle.dump(self, f)\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        \"\"\"Populate the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.init_vocab()\n",
    "        self.add_word(self.start_word)\n",
    "        self.add_word(self.end_word)\n",
    "        self.add_word(self.unk_word)\n",
    "        self.add_captions()\n",
    "\n",
    "    def init_vocab(self):\n",
    "        \"\"\"Initialize the dictionaries for converting tokens to integers (and vice-versa).\"\"\"\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Add a token to the vocabulary.\"\"\"\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def add_captions(self):\n",
    "        \"\"\"Loop over training captions and add all tokens to the vocabulary that meet or exceed the threshold.\"\"\"\n",
    "        coco = COCO(self.annotations_file)\n",
    "        counter = Counter()\n",
    "        ids = coco.anns.keys()\n",
    "        for i, id in enumerate(ids):\n",
    "            caption = str(coco.anns[id]['caption'])\n",
    "            tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "            if i % 100000 == 0:\n",
    "                print(\"[%d/%d] Tokenizing captions...\" % (i, len(ids)))\n",
    "\n",
    "        words = [word for word, cnt in counter.items() if cnt >= self.vocab_threshold]\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx[self.unk_word]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyzIwDAfRrtS"
   },
   "outputs": [],
   "source": [
    "def get_loader(transform,\n",
    "               mode='train',\n",
    "               batch_size=1,\n",
    "               vocab_threshold=None,\n",
    "               vocab_file='./vocab.pkl',\n",
    "               start_word=\"<start>\",\n",
    "               end_word=\"<end>\",\n",
    "               unk_word=\"<unk>\",\n",
    "               vocab_from_file=True,\n",
    "               num_workers=0,\n",
    "               cocoapi_loc='/opt'):\n",
    "    \"\"\"Returns the data loader.\n",
    "    Args:\n",
    "      transform: Image transform.\n",
    "      mode: One of 'train' or 'test'.\n",
    "      batch_size: Batch size (if in testing mode, must have batch_size=1).\n",
    "      vocab_threshold: Minimum word count threshold.\n",
    "      vocab_file: File containing the vocabulary. \n",
    "      start_word: Special word denoting sentence start.\n",
    "      end_word: Special word denoting sentence end.\n",
    "      unk_word: Special word denoting unknown words.\n",
    "      vocab_from_file: If False, create vocab from scratch & override any existing vocab_file.\n",
    "                       If True, load vocab from from existing vocab_file, if it exists.\n",
    "      num_workers: Number of subprocesses to use for data loading \n",
    "      cocoapi_loc: The location of the folder containing the COCO API: https://github.com/cocodataset/cocoapi\n",
    "    \"\"\"\n",
    "    \n",
    "    assert mode in ['train', 'test'], \"mode must be one of 'train' or 'test'.\"\n",
    "    if vocab_from_file==False: assert mode=='train', \"To generate vocab from captions file, must be in training mode (mode='train').\"\n",
    "\n",
    "    # Based on mode (train, val, test), obtain img_folder and annotations_file.\n",
    "    if mode == 'train':\n",
    "        if vocab_from_file==True: assert os.path.exists(vocab_file), \"vocab_file does not exist.  Change vocab_from_file to False to create vocab_file.\"\n",
    "        img_folder ='/content/train2014/'\n",
    "        annotations_file = ('/content/annotations/captions_train2014.json')\n",
    "    if mode == 'test':\n",
    "        assert batch_size==1, \"Please change batch_size to 1 if testing your model.\"\n",
    "        assert os.path.exists(vocab_file), \"Must first generate vocab.pkl from training data.\"\n",
    "        assert vocab_from_file==True, \"Change vocab_from_file to True.\"\n",
    "        img_folder = '/content/train2014/'\n",
    "        annotations_file = ('/content/annotations/image_info_test2014.json')\n",
    "\n",
    "    # COCO caption dataset.\n",
    "    dataset = CoCoDataset(transform=transform,\n",
    "                          mode=mode,\n",
    "                          batch_size=batch_size,\n",
    "                          vocab_threshold=vocab_threshold,\n",
    "                          vocab_file=vocab_file,\n",
    "                          start_word=start_word,\n",
    "                          end_word=end_word,\n",
    "                          unk_word=unk_word,\n",
    "                          annotations_file=annotations_file,\n",
    "                          vocab_from_file=vocab_from_file,\n",
    "                          img_folder=img_folder)\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        initial_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        # data loader for COCO dataset.\n",
    "        data_loader = data.DataLoader(dataset=dataset, \n",
    "                                      num_workers=num_workers,\n",
    "                                      batch_sampler=data.sampler.BatchSampler(sampler=initial_sampler,\n",
    "                                                                              batch_size=dataset.batch_size,\n",
    "                                                                              drop_last=False))\n",
    "    else:\n",
    "        data_loader = data.DataLoader(dataset=dataset,\n",
    "                                      batch_size=dataset.batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "class CoCoDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, transform, mode, batch_size, vocab_threshold, vocab_file, start_word, \n",
    "        end_word, unk_word, annotations_file, vocab_from_file, img_folder):\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab = Vocabulary(vocab_threshold, vocab_file, start_word,\n",
    "            end_word, unk_word, annotations_file, vocab_from_file)\n",
    "        self.img_folder = img_folder\n",
    "        if self.mode == 'train':\n",
    "            self.coco = COCO(annotations_file)\n",
    "            self.ids = list(self.coco.anns.keys())\n",
    "            print('Obtaining caption lengths...')\n",
    "            all_tokens = [nltk.tokenize.word_tokenize(str(self.coco.anns[self.ids[index]]['caption']).lower()) for index in tqdm(np.arange(len(self.ids)))]\n",
    "            self.caption_lengths = [len(token) for token in all_tokens]\n",
    "        else:\n",
    "            test_info = json.loads(open(annotations_file).read())\n",
    "            self.paths = [item['file_name'] for item in test_info['images']]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # obtain image and caption if in training mode\n",
    "        if self.mode == 'train':\n",
    "            ann_id = self.ids[index]\n",
    "            caption = self.coco.anns[ann_id]['caption']\n",
    "            img_id = self.coco.anns[ann_id]['image_id']\n",
    "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # Convert caption to tensor of word ids.\n",
    "            tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "            caption = []\n",
    "            caption.append(self.vocab(self.vocab.start_word))\n",
    "            caption.extend([self.vocab(token) for token in tokens])\n",
    "            caption.append(self.vocab(self.vocab.end_word))\n",
    "            caption = torch.Tensor(caption).long()\n",
    "\n",
    "            # return pre-processed image and caption tensors\n",
    "            return image, caption\n",
    "\n",
    "        # obtain image if in test mode\n",
    "        else:\n",
    "            path = self.paths[index]\n",
    "\n",
    "            # Convert image to tensor and pre-process using transform\n",
    "            PIL_image = Image.open(os.path.join(self.img_folder, path)).convert('RGB')\n",
    "            orig_image = np.array(PIL_image)\n",
    "            image = self.transform(PIL_image)\n",
    "\n",
    "            # return original image and pre-processed image tensor\n",
    "            return orig_image, image\n",
    "\n",
    "    def get_train_indices(self):\n",
    "        sel_length = np.random.choice(self.caption_lengths)\n",
    "        all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]\n",
    "        indices = list(np.random.choice(all_indices, size=self.batch_size))\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 'train':\n",
    "            return len(self.ids)\n",
    "        else:\n",
    "            return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yl7BqNwiR4Lb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cteBlF5rR4U6"
   },
   "outputs": [],
   "source": [
    "# Define a transform to pre-process the training images.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Set the minimum word count threshold.\n",
    "vocab_threshold = 5\n",
    "\n",
    "# Specify the batch size.\n",
    "batch_size = 10\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P35pKv69R9SX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtbDmJUYXGAW"
   },
   "outputs": [],
   "source": [
    "sample_caption = 'A person doing a trick on a rail while riding a skateboard.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zx7L6r0ZaRcJ",
    "outputId": "670c9253-9482-4f2d-d0f8-c2cc442267d1"
   },
   "outputs": [],
   "source": [
    "sample_tokens = nltk.tokenize.word_tokenize(str(sample_caption).lower())\n",
    "print(sample_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PPjnnEvZaYgB",
    "outputId": "a7dacde6-d738-4bba-eeb8-23f01e885f36"
   },
   "outputs": [],
   "source": [
    "sample_caption = []\n",
    "\n",
    "start_word = data_loader.dataset.vocab.start_word\n",
    "print('Special start word:', start_word)\n",
    "sample_caption.append(data_loader.dataset.vocab(start_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "b8gdI3SjaaxP",
    "outputId": "982ba21b-9851-41b5-8fea-adebe20d4ef1"
   },
   "outputs": [],
   "source": [
    "sample_caption.extend([data_loader.dataset.vocab(token) for token in sample_tokens])\n",
    "print(sample_caption)\n",
    "end_word = data_loader.dataset.vocab.end_word\n",
    "print('Special end word:', end_word)\n",
    "\n",
    "sample_caption.append(data_loader.dataset.vocab(end_word))\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_i9mjcVWSBKG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3vDpt8Xracwb",
    "outputId": "f1a1b54c-743c-4136-cb5c-97701afb6f2a"
   },
   "outputs": [],
   "source": [
    "sample_caption = torch.Tensor(sample_caption).long()\n",
    "print(sample_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "4Q3ntgMWaisK",
    "outputId": "4964f895-21c5-4cde-d783-7f9b90f017f5"
   },
   "outputs": [],
   "source": [
    "dict(list(data_loader.dataset.vocab.word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2Ih8j_Dias2W",
    "outputId": "53fd02cf-6cb6-4510-f050-46b94527c885"
   },
   "outputs": [],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "3YOR8YJSawk2",
    "outputId": "73e164ef-47df-4351-bf75-12438b10f252"
   },
   "outputs": [],
   "source": [
    "# Modify the minimum word count threshold.\n",
    "vocab_threshold = 4\n",
    "\n",
    "# Obtain the data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GT-KKejkazRm",
    "outputId": "ece4e85b-79cb-4935-8f9a-16cb3007d4e8"
   },
   "outputs": [],
   "source": [
    "# Print the total number of keys in the word2idx dictionary.\n",
    "print('Total number of tokens in vocabulary:', len(data_loader.dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xwf9E4Sla2av",
    "outputId": "0ced51a9-e6b5-415a-dcab-619310ae84a7"
   },
   "outputs": [],
   "source": [
    "unk_word = data_loader.dataset.vocab.unk_word\n",
    "print('Special unknown word:', unk_word)\n",
    "\n",
    "print('All unknown words are mapped to this integer:', data_loader.dataset.vocab(unk_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gRRan90Ta419",
    "outputId": "a663a848-f97e-4a63-840c-5ab2c8be35e4"
   },
   "outputs": [],
   "source": [
    "print(data_loader.dataset.vocab('jfkafejw'))\n",
    "print(data_loader.dataset.vocab('ieowoqjf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "0TR-uAXga6di",
    "outputId": "6347b89a-6bf6-4b00-d9e6-b9b10c7f30fd"
   },
   "outputs": [],
   "source": [
    "# Obtain the data loader (from file). Note that it runs much faster than before!\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_from_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "TkpRJ21ia-ZI",
    "outputId": "9ad43027-259a-4a0c-84c7-9b3b83ab0f5d"
   },
   "outputs": [],
   "source": [
    "# Tally the total number of training captions with each length.\n",
    "counter = Counter(data_loader.dataset.caption_lengths)\n",
    "lengths = sorted(counter.items(), key=lambda pair: pair[1], reverse=True)\n",
    "for value, count in lengths:\n",
    "    print('value: %2d --- count: %5d' % (value, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "eL5mRZFma-q8",
    "outputId": "84e0853a-b6c7-48f8-904f-dae4b7cf0d72"
   },
   "outputs": [],
   "source": [
    "# Randomly sample a caption length, and sample indices with that length.\n",
    "indices = data_loader.dataset.get_train_indices()\n",
    "print('sampled indices:', indices)\n",
    "\n",
    "# Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "data_loader.batch_sampler.sampler = new_sampler\n",
    "    \n",
    "# Obtain the batch.\n",
    "images, captions = next(iter(data_loader))\n",
    "\n",
    "print('images.shape:', images.shape)\n",
    "print('captions.shape:', captions.shape)\n",
    "\n",
    "# (Optional) Uncomment the lines of code below to print the pre-processed images and captions.\n",
    "# print('images:', images)\n",
    "# print('captions:', captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KxJu1tMhSMo0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47X2jiK2SMs-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqWAOL5qSMuz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPq-uyqBSMwl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShWLisWxSMyJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2ZS4D2sol-U"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-50 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        ################ TODO: load pretrained resnet model ################\n",
    "        resnet = \n",
    "        modules = list(resnet.children())[:-1] \n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        features = self.bn(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        ################ TODO: fill-in decoder network architecture ################\n",
    "        self.embed = \n",
    "        self.lstm = \n",
    "        self.linear = \n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        captions = captions[:,:-1]\n",
    "        embeddings = self.embed(captions)\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, inputs, states=None, max_len=20):\n",
    "        \"\"\"Accept a pre-processed image tensor (inputs) and return predicted \n",
    "        sentence (list of tensor ids of length max_len). This is the greedy\n",
    "        search approach.\n",
    "        \"\"\"\n",
    "        sampled_ids = []\n",
    "        for i in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            # Get the index (in the vocabulary) of the most likely integer that\n",
    "            # represents a word\n",
    "            predicted = outputs.argmax(1)\n",
    "            sampled_ids.append(predicted.item())\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        return sampled_ids\n",
    "\n",
    "    def sample_beam_search(self, inputs, states=None, max_len=20, beam_width=5):\n",
    "        \"\"\"Accept a pre-processed image tensor and return the top predicted \n",
    "        sentences. This is the beam search approach.\n",
    "        \"\"\"\n",
    "        # Top word idx sequences and their corresponding inputs and states\n",
    "        idx_sequences = [[[], 0.0, inputs, states]]\n",
    "        for _ in range(max_len):\n",
    "            # Store all the potential candidates at each step\n",
    "            all_candidates = []\n",
    "            # Predict the next word idx for each of the top sequences\n",
    "            for idx_seq in idx_sequences:\n",
    "                hiddens, states = self.lstm(idx_seq[2], idx_seq[3])\n",
    "                outputs = self.linear(hiddens.squeeze(1))\n",
    "                # Transform outputs to log probabilities to avoid floating-point \n",
    "                # underflow caused by multiplying very small probabilities\n",
    "                log_probs = F.log_softmax(outputs, -1)\n",
    "                top_log_probs, top_idx = log_probs.topk(beam_width, 1)\n",
    "                top_idx = top_idx.squeeze(0)\n",
    "                # create a new set of top sentences for next round\n",
    "                for i in range(beam_width):\n",
    "                    next_idx_seq, log_prob = idx_seq[0][:], idx_seq[1]\n",
    "                    next_idx_seq.append(top_idx[i].item())\n",
    "                    log_prob += top_log_probs[0][i].item()\n",
    "                    # Indexing 1-dimensional top_idx gives 0-dimensional tensors.\n",
    "                    # We have to expand dimensions before embedding them\n",
    "                    inputs = self.embed(top_idx[i].unsqueeze(0)).unsqueeze(0)\n",
    "                    all_candidates.append([next_idx_seq, log_prob, inputs, states])\n",
    "            # Keep only the top sequences according to their total log probability\n",
    "            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "            idx_sequences = ordered[:beam_width]\n",
    "        return [idx_seq[0] for idx_seq in idx_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "plYc5x3jbBNb",
    "outputId": "4ef09b73-1a93-45a4-85d9-6f0727674d7a"
   },
   "outputs": [],
   "source": [
    "# Watch for any changes in model.py, and re-load it automatically.\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "\n",
    "# Import EncoderCNN and DecoderRNN. \n",
    "# Specify the dimensionality of the image embedding.\n",
    "embed_size = 256\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Initialize the encoder. (Optional: Add additional arguments if necessary.)\n",
    "encoder = EncoderCNN(embed_size)\n",
    "\n",
    "# Move the encoder to GPU if CUDA is available.\n",
    "encoder.to(device)\n",
    "    \n",
    "# Move last batch of images (from Step 2) to GPU if CUDA is available.   \n",
    "images = images.to(device)\n",
    "\n",
    "# Pass the images through the encoder.\n",
    "features = encoder(images)\n",
    "\n",
    "print('type(features):', type(features))\n",
    "print('features.shape:', features.shape)\n",
    "\n",
    "# Check that your encoder satisfies some requirements of the project! :D\n",
    "assert type(features)==torch.Tensor, \"Encoder output needs to be a PyTorch Tensor.\" \n",
    "assert (features.shape[0]==batch_size) & (features.shape[1]==embed_size), \"The shape of the encoder output is incorrect.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IbF4K-_SbDmr",
    "outputId": "ddc8029c-7e50-4e4c-8991-6294cd5fb962"
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "\n",
    "# Store the size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the decoder.\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size, 1)\n",
    "\n",
    "# Move the decoder to GPU if CUDA is available.\n",
    "decoder.to(device)\n",
    "    \n",
    "# Move last batch of captions (from Step 1) to GPU if CUDA is available \n",
    "captions = captions.to(device)\n",
    "\n",
    "# Pass the encoder output and captions through the decoder.\n",
    "outputs = decoder(features, captions)\n",
    "\n",
    "print('type(outputs):', type(outputs))\n",
    "print('outputs.shape:', outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Ba2M9fu-QrVq",
    "outputId": "32b1effe-436a-4967-8fe6-a98abad2933e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "#import sys\n",
    "#sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "#from data_loader import get_loader\n",
    "#from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "# Select appropriate values for the Python variables below.\n",
    "batch_size = 64          # batch size\n",
    "vocab_threshold = 4        # minimum word count threshold\n",
    "vocab_from_file = True   # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 2             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "eOz_L6EvpKoS",
    "outputId": "1bad0267-03c9-4721-9cd8-00d46f5b635d"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "\n",
    "old_time = time.time()\n",
    "# response = requests.request(\"GET\", \n",
    "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                             headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "                # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TechXADL_Lab_Day7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
